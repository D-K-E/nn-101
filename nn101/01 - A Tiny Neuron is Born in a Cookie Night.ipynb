{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks From Scratch - 01 - A Tiny Neuron is Born in a Cookie Night\n",
    "\n",
    "We are going to implement neural networks from scratch, that is no tensorflow, pytorch, scikit-learn etc.\n",
    "\n",
    "Most of the material is taken from my another [repository](https://gitlab.com/QmAuber/basit-ysa-python/-/blob/master/ysa/defterler/YapayZekaMatematik.ipynb)\n",
    "\n",
    "Here is the list of packages we will be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np ## for matrix operations, you can excange it with something else\n",
    "# or implement your own array operations, we don't use anything fancy here\n",
    "from typing import List, Tuple, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives\n",
    "\n",
    "The first thing you need to understand when implementing Neural Networks is by and large, derivatives. \n",
    "So what are derivatives ? Grossly reduced, they are operators that help us to understand how functions behave. Notice that derivative**s**, so there are more than one type.\n",
    "\n",
    "The simplest derivative can be expressed mathematically as:\n",
    "$$f'(x) = \\lim_{h\\to 0} \\frac{f(x+h) - f(x)}{h}$$\n",
    "\n",
    "Here we assume that: $f: \\mathbb{R} \\to \\mathbb{R}$. You can read this as the following:\n",
    "\n",
    "*Function f takes in a real number and outputs a real number*\n",
    "\n",
    "In code this would make:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(f: Callable[[float], float], x: float, h=1e-8) -> float:\n",
    "    \"Takes the derivative of f\"\n",
    "    f_t = (f(x + h) - f(x)) / h\n",
    "    return f_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply put I add a tiny bit of something to the argument of f and apply the f to both the original and the added argument, then observe the difference relative to added value. The more I do this, I understand better and better how the function behaves.\n",
    "\n",
    "Well this is all and good but what if ... just what if the function f takes in multiple values, like a list of values ?\n",
    "So mathematically what if the definition of $f$ is something like the following:\n",
    "\n",
    "$$f: \\mathbb{R}^N \\to R$$\n",
    "\n",
    "If this is the case, since I only have a single $h$ to add, this raises a question. Which value should receive the $h$ while taking the derivative ? The answer is whichever you want. \n",
    "But then we would not be able to observe *completely* how the function behaves: YES, that is why this way of taking the derivative is called **partial** derivative, and it is specified as the following: \n",
    "$$f'(x_1, ..., x_n) = \\lim_{h\\to0} \\frac{f(x_1, ..., x_i+h, ..., x_n) - f(x_1, ..., x_n)}{h}$$\n",
    "\n",
    "In code this would make:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_derivative(f: Callable[[List[float]], float], \n",
    "                arguments: list, \n",
    "                argument_position: int, \n",
    "                h=1e-8) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Basic implementation of partial derivative\n",
    "    \"\"\"\n",
    "    arguments_copy = arguments.copy() # in order to leave original arguments untouched\n",
    "    arguments_copy[argument_position] = arguments_copy[argument_position] + h\n",
    "    return (f(arguments_copy) - f(arguments)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well what if I want to observe, the **complete** behaviour of the function $f$.\n",
    "Then I basically apply the partial derivative to all of the arguments. This is called taking the **gradient** of the function.\n",
    "\n",
    "In code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(f: Callable[[List[float]], float], \n",
    "                arguments: list, \n",
    "                h=1e-8) -> List[float]:\n",
    "    \"\"\"\n",
    "    Basic implementation of gradient computation\n",
    "    \"\"\"\n",
    "    partials: List[float] = []\n",
    "    for argument_pos, argument in enumerate(arguments):\n",
    "        partial = partial_derivative(f=f, arguments=arguments, argument_position=argument_pos)\n",
    "        partials.append(partial)\n",
    "    return partials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x^2 derivative = 2x => 5^2 derivative 2 * 5 True\n"
     ]
    }
   ],
   "source": [
    "# simple derivative test\n",
    "def testfn(x): return x * x\n",
    "d = derivative(testfn, x=5)\n",
    "print(\"x^2 derivative = 2x => 5^2 derivative 2 * 5\", \n",
    "      10 == round(d, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x,y) = x^2 + xy + y^2, x=2, y=6\n",
      "f_x'(x,y) = 2x + y => 4 + 6 = 10\n",
      "f_y'(x,y) = 2y + x => 2 + 12 = 14\n",
      "f_x'(x,y) = 10 True\n",
      "f_y'(x,y) = 14 True\n"
     ]
    }
   ],
   "source": [
    "# testing partial derivative\n",
    "def test_part_fn(xs: list): return (xs[0] ** 2) + (xs[0] * xs[1]) + (xs[1] ** 2)\n",
    "# x^2 + xy + y^2\n",
    "# f_x' = 2x + y , f_y' = 2y + x\n",
    "xd = partial_derivative(f=test_part_fn, arguments=[2, 6], argument_position=0)\n",
    "yd = partial_derivative(f=test_part_fn, arguments=[2, 6], argument_position=1)\n",
    "print(\"f(x,y) = x^2 + xy + y^2, x=2, y=6\")\n",
    "print(\"f_x'(x,y) = 2x + y => 4 + 6 = 10\")\n",
    "print(\"f_y'(x,y) = 2y + x => 2 + 12 = 14\")\n",
    "print(\"f_x'(x,y) = 10\", round(xd, 3) == 10)\n",
    "print(\"f_y'(x,y) = 14\", round(yd, 3) == 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "∇f(x,y), gradient of f, that is [f_x', f_y'] => [10, 14]\n",
      "∇f(x,y)= [10, 14],  True\n"
     ]
    }
   ],
   "source": [
    "# test gradient\n",
    "partials = gradient(f=test_part_fn, arguments=[2, 6])\n",
    "print(\"∇f(x,y), gradient of f, that is [f_x', f_y'] => [10, 14]\")\n",
    "print(\"∇f(x,y)= [10, 14], \", [round(p, 3) for p in partials] == [10, 14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if the signature of the function $f$ is the following:\n",
    "$$f: \\mathbb{R} \\to \\mathbb{R}^n$$\n",
    "\n",
    "This is called a vector function, simply a function that outputs a vector. It can also be interpreted as $f(x) = [f_1(x), f_2(x), \\dots, f_n(x)]$, that is we can interpret it as a set of functions instead of a single function.\n",
    "\n",
    "When taking the derivative of this function, we have two cases:\n",
    "\n",
    "- The member functions, let's call them, sub functions of $f$ are known\n",
    "- The member functions of $f$ are not known\n",
    "\n",
    "When we know the member function, the derivative is simply:\n",
    "$$f'(x) = [f'_1(x), f'_2(x), \\dots, f'_n(x)]$$\n",
    "\n",
    "When we don't know the member functions, the derivative is:\n",
    "\n",
    "$$f'(x) = \\lim_{h\\to 0} \\frac{f(x+h) - f(x)}{h}$$\n",
    "\n",
    "The only difference here is that the division is a division of a vector by a scalar.\n",
    "\n",
    "So in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_of_vector_function_with_known_functions(\n",
    "    fs: List[Callable[[float], float]], \n",
    "    x: float,\n",
    "    h=1e-8\n",
    ")-> List[float]:\n",
    "    \"Compute derivative of a vector function when member functions are known\"\n",
    "    derivatives = []\n",
    "    for f in fs:\n",
    "        d = derivative(f, x)\n",
    "        derivatives.append(d)\n",
    "    return derivatives\n",
    "\n",
    "def derivative_of_vector_function(\n",
    "    f: Callable[[float], List[float]], \n",
    "    x: float, \n",
    "    h=1e-8) -> List[float]:\n",
    "    \"Derivative of a vector function where member functions are not known\"\n",
    "    return list((np.array(f(x + h)) - np.array(f(x))) / h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test all that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with known functions True\n",
      "with unknown functions True\n"
     ]
    }
   ],
   "source": [
    "# testler\n",
    "def testfn(x): return [x*x, 2 * x, x - 1]\n",
    "# turevi bunun 2x, 2, 1\n",
    "vk = derivative_of_vector_function_with_known_functions(fs=[lambda x: x*x, \n",
    "                                     lambda x: 2 * x, \n",
    "                                     lambda x: x -1],\n",
    "                                 x=3)\n",
    "print(\"with known functions\", \n",
    "      [round(v, 3) for v in vk] == [6, 2, 1])\n",
    "vnk = derivative_of_vector_function(f=testfn, x=3)\n",
    "print(\"with unknown functions\", \n",
    "      [round(v, 3) for v in vnk] == [6, 2, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have covered the following function signatures:\n",
    "\n",
    "- $f: \\mathbb{R} \\to \\mathbb{R}$ - goes with simple derivative\n",
    "- $f: \\mathbb{R}^n \\to \\mathbb{R}$ - goes with partial derivative and the gradient\n",
    "- $f: \\mathbb{R} \\to \\mathbb{R}^n$ - goes with vector derivative\n",
    "\n",
    "Now what if the function signature is the last conceivable option, that is:\n",
    "\n",
    "- $f: \\mathbb{R}^n \\to \\mathbb{R}^m$\n",
    "\n",
    "How would we compute the derivative of such a function ?\n",
    "The logic is a mix of what we do with vector functions and gradient.\n",
    "\n",
    "Again we can interpret the $f$ as a set of functions whose number of elements equals to $m$ where each function takes $n$ arguments. \n",
    "Basically as the following matrix:\n",
    "\n",
    "$f(x) = \\pmatrix{\n",
    "f_1(x_1, ..., x_n)  \\\\\n",
    "f_2(x_1, ..., x_n) \\\\\n",
    "... \\\\\n",
    "f_m(x_1, ..., x_n)\n",
    "}$\n",
    "\n",
    "Then the derivative of such a function would simply be:\n",
    "$$f'(x)= J(f) = \\pmatrix{\n",
    "f'_1(x_1, ..., x_n)  \\\\\n",
    "f'_2(x_1, ..., x_n) \\\\\n",
    "... \\\\\n",
    "f'_m(x_1, ..., x_n)\n",
    "}$$\n",
    "\n",
    "If we make it even more explicit:\n",
    "\n",
    "$$J(f) = \\pmatrix{\n",
    "f_{x_1, 1}'(x) = \\frac{f(x_1+h, ..., x_n) - f(x)}{h} & ... & f_{x_n, 1}'(x) = \\frac{f(x_1, ..., x_n+h) - f(x)}{h} \\\\\n",
    "f_{x_1, 2}'(x) = \\frac{f(x_1+h, ..., x_n) - f(x)}{h} & ... & f_{x_n, 2}'(x) = \\frac{f(x_1, ..., x_n+h) - f(x)}{h} \\\\\n",
    "\\vdots & ... & \\vdots \\\\\n",
    "f_{x_1, m}'(x) = \\frac{f(x_1+h, ..., x_n) - f(x)}{h} & ... & f_{x_n, m}'(x) = \\frac{f(x_1, ..., x_n+h) - f(x)}{h}\n",
    "}$$\n",
    "\n",
    "Here is an example: $ f(x,y,z)=[(2x+y),3z/2,y2+z,6x]$. \n",
    "\n",
    "The derivative matrix of $f$ would be:\n",
    "$$J(f) = \\pmatrix{\n",
    "2 & 1 & 0 \\\\\n",
    "0 & 0 & \\frac{3}{2} \\\\\n",
    "0 & 2y & 1 \\\\\n",
    "6 & 0 & 0\n",
    "}$$\n",
    "\n",
    "This derivative matrix is called **Jacoby matrix**. Computing this matrix is informally called taking/evaluating the jacobian of a function.\n",
    "\n",
    "The previous question arises again: What if we don't know the member functions ?\n",
    "\n",
    "Then taking the jacobian means simply:\n",
    "\n",
    "$$f'(x) = \\lim_{h\\to 0} \\frac{f(x+h) - f(x)}{h}$$\n",
    "\n",
    "The only difference is that we divide a matrix by a scalar at the end.\n",
    "\n",
    "In code this would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacoby_with_known_functions(fs: List[Callable[[List[float]], float]], \n",
    "           args: List[float], h=1e-8) -> List[List[float]]:\n",
    "    j_mat = []\n",
    "    for f in fs:\n",
    "        d = gradient(f=f, arguments=args, h=h)\n",
    "        j_mat.append(d)\n",
    "    return j_mat\n",
    "\n",
    "def jacoby_pure_python(f: Callable[[List[float]], List[float]], \n",
    "           args: List[float], \n",
    "           h=1e-8) -> List[List[float]]:\n",
    "    \"\"\n",
    "    nargs = [a for a in args] \n",
    "    j_mat = []\n",
    "    for i, a in enumerate(args):\n",
    "        narg = a + h\n",
    "        nargs[i] = narg\n",
    "        d = list((np.array(f(nargs)) - np.array(f(args))) / h)\n",
    "        j_mat.append(d)\n",
    "        nargs[i] = a\n",
    "    return j_mat\n",
    "\n",
    "def jacobian(f: Callable[[np.ndarray], np.ndarray], \n",
    "                args: np.ndarray, h=1e-8) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute jacobian with numpy\n",
    "    \"\"\"\n",
    "    result = f(args)\n",
    "    j_mat = np.zeros((*args.shape, *result.shape), dtype=np.float)\n",
    "    for index in np.ndindex(args.shape):\n",
    "        narg = args.copy()\n",
    "        narg[index] += h\n",
    "        j_mat[index] = (f(narg) - f(args)) / h\n",
    "    return j_mat.reshape((result.size, args.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jacobian with known functions:  [[2.  1.  0. ]\n",
      " [0.  0.  1.5]\n",
      " [0.  2.  1. ]\n",
      " [6.  0.  0. ]]\n",
      "jacobian with unknown function:  [[2.  0.  0.  6. ]\n",
      " [1.  0.  2.  0. ]\n",
      " [0.  1.5 1.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "def f1(arg: List[float]): return 2*arg[0] + arg[1]\n",
    "def f2(arg: List[float]): return (3 * arg[2]) / 2\n",
    "def f3(arg: List[float]): return arg[1]*arg[1] + arg[2]\n",
    "def f4(arg: List[float]): return 6*arg[0]\n",
    "        \n",
    "def testfn(arg: List[float]): \n",
    "    return [\n",
    "    f1(arg), \n",
    "    f2(arg),\n",
    "    f3(arg),\n",
    "    f4(arg)\n",
    "]\n",
    "j_known = jacoby_with_known_functions(\n",
    "    fs=[f1,f2,f3,f4], args=[2,1, 3])\n",
    "print(\"jacobian with known functions: \",\n",
    "      np.around(np.array(j_known), 3))\n",
    "j_unknown = jacoby_pure_python(f=testfn, args=[2,1,3])\n",
    "print(\"jacobian with unknown function: \", np.around(np.array(j_unknown), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jacobian with numpy: \n",
      "[[2. 0. 0. 2. 0. 0. 2. 0. 0. 2. 0. 0.]\n",
      " [0. 3. 0. 0. 3. 0. 0. 3. 0. 0. 3. 0.]\n",
      " [0. 0. 4. 0. 0. 4. 0. 0. 4. 0. 0. 4.]]\n",
      "jacobian with numpy: \n",
      "[[2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 4. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 4. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 4. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 4.]]\n"
     ]
    }
   ],
   "source": [
    "def testfn1(arg):\n",
    "    return np.array([arg[0].sum() * 2, \n",
    "                     arg[1].sum() * 3, \n",
    "                     arg[2].sum() * 4], \n",
    "                    dtype=np.float)\n",
    "\n",
    "def testfn2(arg):\n",
    "    return np.array([arg[0] * 2, \n",
    "                     arg[1] * 3, \n",
    "                     arg[2] * 4], \n",
    "                    dtype=np.float)\n",
    "\n",
    "\n",
    "j_np_n1 = jacobian(f=testfn1, \n",
    "                      args=np.array([[1, 2, 3, 4], \n",
    "                                           [5, 6, 7, 8],\n",
    "                                           [9, 10, 11, 12]\n",
    "                                          ], \n",
    "                                          dtype=np.float)\n",
    "                     )\n",
    "\n",
    "j_np_n2 = jacobian(f=testfn2, \n",
    "                     args=np.array(\n",
    "                         [[1, 2, 3, 4],\n",
    "                          [5, 6, 7, 8],\n",
    "                          [9, 10, 11, 12]\n",
    "                         ], \n",
    "                         dtype=np.float)\n",
    "                    )\n",
    "\n",
    "\n",
    "print(\"jacobian with numpy: \") \n",
    "print(np.around(j_np_n1, 3))\n",
    "\n",
    "print(\"jacobian with numpy: \")\n",
    "print(np.around(j_np_n2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have one last option to consider and that is composite functions.\n",
    "\n",
    "Armed with the knowledge of jacobian, we can apply the derivative to most of the functions in everyday use. However what if our function is composed of a sequential application of a series of functions, so what if it is something like: $f(x) = k(p(x))$\n",
    "\n",
    "Notice that this is not truly the same case with the vector functions or the jacobian. \n",
    "In jacobian, or vector functions, the order of evaluation, or let's say the order in which we apply the functions or the derivative does not change the result. \n",
    "Here we have a clear sense of time, a sequential relation. The function $p$ has to be evaluated first.\n",
    "\n",
    "In any case how do we compute its derivative ?\n",
    "Let's start with the simplest case again:\n",
    "\n",
    "Suppose that $f,k,p: \\mathbb{R} \\to \\mathbb{R}$\n",
    "\n",
    "The derivative of $f$ is defined by the following:\n",
    "\n",
    "$$f'(x) = k'( p(x) ) * p'(x)$$\n",
    "\n",
    "This equivalence is also known as the **chain rule**.\n",
    "\n",
    "Let's see that in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain1d(f: Callable[[float], float], g: Callable[[float], float], x: float):\n",
    "    \"\"\n",
    "    delta_g = derivative(g, x)\n",
    "    g_value = g(x)\n",
    "    delta_f = derivative(f, g_value)\n",
    "    return delta_f * delta_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have equal derivatives, right ?  True\n"
     ]
    }
   ],
   "source": [
    "# chain test\n",
    "def f(x): return 2 * x\n",
    "def g(x): return x * x\n",
    "# derivative 2 * 2x = 4x =1\n",
    "def testfn(x): return f(g(x))\n",
    "print(\"We have equal derivatives, right ? \", \n",
    "      round(derivative(testfn, x=2), 3) == round(chain1d(f=f, g=g, x=2), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again what if our function signature is $f: \\mathbb{R}^n \\to \\mathbb{R}^m$\n",
    "where the interior signatures are something like: \n",
    "- $p:\\mathbb{R}^n \\to \\mathbb{R}^z$\n",
    "- $k:\\mathbb{R}^z \\to \\mathbb{R}^m$\n",
    "\n",
    "Well by the hand of God, the chain rule is strictly the same, we just use the Jacobian:\n",
    "$f'(x) = J_k(g(x)) * J_g(x)$\n",
    "\n",
    "In code this would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain(f: Callable[[np.ndarray], np.ndarray], \n",
    "                    g: Callable[[np.ndarray], np.ndarray],\n",
    "                    x: np.ndarray\n",
    "                   ) -> np.ndarray:\n",
    "        \"\"\n",
    "        g_val = g(x)\n",
    "        g_j = jacobian(f=g, args=x)\n",
    "        f_j = jacobian(f=f, args=g_val)\n",
    "        return np.matmul(f_j, g_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.5 0.  0. ]\n",
      " [0.5 0.  0.5]\n",
      " [0.  2.  0. ]\n",
      " [0.  4.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "# test jacoby\n",
    "# test fn\n",
    "def f(x: np.ndarray): return np.array([x[0]*3 + 2, x[1], x[2]*2, \n",
    "                                       x[0] + x[1]], dtype=np.float)\n",
    "\n",
    "def g(x: np.ndarray): return np.array([x[0] / 2, x[1]*2, x[2]/2], dtype=np.float)\n",
    "\n",
    "args = np.array([3, 1, 3], dtype=np.float)\n",
    "jmat = chain(f, g, args)\n",
    "print(np.around(jmat, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation Graphs\n",
    "\n",
    "Here is the last bit of knowledge required for understanding neural networks. \n",
    "It turns out, composing functions are quite cool, and that we can express almost anything in terms of composite functions. \n",
    "\n",
    "Wouldn't it be better if we can represent function composition in an intuitive way. \n",
    "Well computation graphs are mostly that, meaning a nice a way of expressing sequence of computations. \n",
    "They also allow us to know the general form of the computation before evaluation which gives you a margin for optimizing and/or serializing the computation.\n",
    "\n",
    "From the perspective of graph theory, computation graphs are almost always acyclic and directed. \n",
    "\n",
    "So basically if we have $f(x) = g(k(p(x)))$\n",
    "We represent it as:\n",
    "```\n",
    "   p         k          g\n",
    "x ---> x_p -----> x_k ----> x_g\n",
    "\n",
    "```\n",
    "If we take the derivative of the function $f$, the resulting computational graph would be something like:\n",
    "\n",
    "```\n",
    "\n",
    "   p         k             g\n",
    "x ---> x_p -------> x_k ------------> x_g\n",
    "                                       /\n",
    "       p'(x)          k'(x)         g'(x)\n",
    "x'_p <-------- x'_k <------- x'_g <------ \n",
    "```\n",
    "\n",
    "So each node of this graph is a computation. In code that would make:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractComputation:\n",
    "    def __init__(self):\n",
    "        self.args = None\n",
    "        \n",
    "    def result(self, arg: np.ndarray):\n",
    "        raise NotImplemented\n",
    "        \n",
    "    def delta(self):\n",
    "        raise NotImplemented\n",
    "        \n",
    "    @property\n",
    "    def output(self):\n",
    "        raise NotImplemented\n",
    "        \n",
    "    @property\n",
    "    def d(self):\n",
    "        raise NotImplemented\n",
    "        \n",
    "class Computation(AbstractComputation):\n",
    "    def __init__(self, inpt=None, function=lambda x: x):\n",
    "        super().__init__()\n",
    "        self.args = inpt\n",
    "        self.f = function\n",
    "        \n",
    "    def __str__(self):\n",
    "        return (\"Computation Node:\\nInput: \" \n",
    "                + str(self.args.shape) \n",
    "                + \"\\nOutput:\" + str(self.output)\n",
    "                + \"\\nFunction\" + str(self.f)\n",
    "               )\n",
    "    @property\n",
    "    def output(self):\n",
    "        return self.compute()\n",
    "    \n",
    "    @property\n",
    "    def d(self):\n",
    "        return jacobian(f=self.f, args=self.args)\n",
    "        \n",
    "    def result(self, arg):\n",
    "        return self.f(arg)\n",
    "    \n",
    "    def compute(self):\n",
    "        res = self.result(arg=self.args)\n",
    "        return res.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node output:  [1. 9. 4.]\n",
      "node derivative [[2. 0. 0.]\n",
      " [0. 6. 0.]\n",
      " [0. 0. 4.]]\n"
     ]
    }
   ],
   "source": [
    "# test computation node\n",
    "n = Computation()\n",
    "n.args = np.array([1, 3, 2], dtype=np.float)\n",
    "n.f = lambda x: x ** 2\n",
    "print(\"node output: \", n.output)  # 1 9 4\n",
    "print(\"node derivative\", np.around(n.d, 3)) # 2 6 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A layer is an instance of a computational graph. In the simplest case, it is a sequence of computations. \n",
    "\n",
    "A network can be composed of more than one layers.\n",
    "\n",
    "However let's implement the layer as a computation node as well, since we ultimately we require the same set of operations from them anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(AbstractComputation):\n",
    "    def __init__(self, flst: List[Callable[[np.ndarray], np.ndarray]], \n",
    "                 arg: np.ndarray):\n",
    "        \"Genel hesap katmani (layer)\"\n",
    "        self.args = arg\n",
    "        self.computations = [Computation(function=f) for f in flst]\n",
    "        self.is_computed = False\n",
    "        \n",
    "    def compute(self):\n",
    "        previous = self.args.copy()\n",
    "        current = self.computations[0]\n",
    "        current.args = previous\n",
    "        for n in self.computations[1:]:\n",
    "            n.args = current.output\n",
    "            current = n\n",
    "        self.is_computed = True\n",
    "        \n",
    "    def compute_until(self, limit: int):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if limit > len(self.computations):\n",
    "            raise ValueError(\n",
    "                \"Limit out of bounds for available number of computations \" + str(limit)\n",
    "                + \" limit \" + str(len(self.computations))\n",
    "            )\n",
    "        ns = self.computations[:limit]\n",
    "        previous_args = self.args.copy()\n",
    "        current = ns[0]\n",
    "        current.args = previous_args\n",
    "        for n in ns[1:]:\n",
    "            n.args = current.output\n",
    "            current = n\n",
    "        return ns\n",
    "    \n",
    "    def compute_until_with_arg(self, limit: int, args: np.ndarray):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if limit > len(self.computations):\n",
    "            raise ValueError(\n",
    "                \"Limit out of bounds for available number of computations \" + str(limit)\n",
    "                + \" limit \" + str(len(self.computations))\n",
    "            )\n",
    "        ns = self.computations[:limit]\n",
    "        previous_args = args.copy()\n",
    "        current = ns[0]\n",
    "        current.args = previous_args\n",
    "        for n in ns[1:]:\n",
    "            n.args = current.output\n",
    "            current = n\n",
    "        return ns\n",
    "    \n",
    "    def delta_from(self, start: int):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.compute()\n",
    "        if start >= len(self.computations):\n",
    "            raise ValueError(\"Start out of bounds \" + str(start) +\" number of \"+\n",
    "                            \" computations: \"+ str(len(self.computations)))\n",
    "        current = self.computations[start]\n",
    "        current_d = current.d\n",
    "        for n in self.computations[start+1:]:\n",
    "            d = n.d\n",
    "            current_d = np.matmul(d,current_d)\n",
    "        return current_d\n",
    "            \n",
    "    @property\n",
    "    def output(self):\n",
    "        self.compute()\n",
    "        return self.computations[-1].output\n",
    "    \n",
    "    @property\n",
    "    def delta_backwards(self):\n",
    "        \"backward accumulation of derivative\"\n",
    "        self.compute()\n",
    "        reversed_cs = list(reversed(self.computations))\n",
    "        last = reversed_cs[0]\n",
    "        ld = last.d\n",
    "        for n in reversed_cs[1:]:\n",
    "            d = n.d\n",
    "            ld = np.matmul(ld, d)\n",
    "        return ld\n",
    "    \n",
    "    @property\n",
    "    def delta(self):\n",
    "        \"forward accumulation of derivative\"\n",
    "        return self.delta_from(start=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.  5. 14.]\n",
      "[[2. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 6.]]\n",
      "[[2. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 6.]]\n"
     ]
    }
   ],
   "source": [
    "# test layer\n",
    "def mx(x: np.ndarray): return x * x \n",
    "def gx(x: np.ndarray): return x + 5\n",
    "layer = Layer(flst=[mx, gx], arg=np.array([1,0,3], dtype=np.float))\n",
    "\"\"\"\n",
    "f'(x): [\n",
    " [2. 0. 0.]\n",
    " [0. 0. 0.]\n",
    " [0. 0. 6.]\n",
    " ]\n",
    "g'(x):  [\n",
    " [1. 0. 0.]\n",
    " [0. 1. 0.]\n",
    " [0. 0. 1.]\n",
    " ]\n",
    "\"\"\"\n",
    "\n",
    "print(layer.output)\n",
    "print(np.around(layer.delta_backwards, 3))\n",
    "print(np.around(layer.delta, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a linear regression layer, just to get a feel for how to do familiar computations with computation graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionLayer(Layer):\n",
    "    def __init__(self, arg: np.ndarray):\n",
    "        \"\"\n",
    "        self.mx = np.random.randn(*arg.shape)\n",
    "        self.b = np.zeros_like(self.mx)\n",
    "        def m_x(x: np.ndarray): return x * self.mx\n",
    "        def b_x(x: np.ndarray): return x + self.b\n",
    "        def mx_b(x: np.ndarray): return b_x(m_x(x))\n",
    "        super().__init__(flst=[m_x, b_x], arg=arg.copy())\n",
    "        \n",
    "    @property\n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        y = mx + b = f(x)\n",
    "        y = f(x)\n",
    "        mx = M(x)\n",
    "        +b = B(x)\n",
    "        f(x) = B(M(x)) = mx + b\n",
    "        \"\"\"\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg = np.array([[6, 5], [2, 1], [9, 3]], dtype=np.float)\n",
    "LRL = LinearRegressionLayer(arg=arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ideal values\n",
    "lrl_slope = np.array([[0.5, 0.5], [0.5, 0.5], [3, 3]], dtype=np.float)\n",
    "lrl_intercept = np.array([[1,1], [1,1], [0,0]], dtype=np.float)\n",
    "lrl_target = np.array([[4, 3.5], [2, 1.5], [27, 9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: \n",
      " [[ 2.68317169 -5.32191027]\n",
      " [ 2.6374344   0.28760089]\n",
      " [-9.23522885  2.71788954]]\n",
      "expected output: \n",
      " [[ 4.   3.5]\n",
      " [ 2.   1.5]\n",
      " [27.   9. ]]\n",
      "derivative matrix: \n",
      " [[ 0.447 -1.064]\n",
      " [ 1.319  0.288]\n",
      " [-1.026  0.906]]\n",
      "expected derivative matrix: \n",
      " [[0.5 0.5]\n",
      " [0.5 0.5]\n",
      " [3.  3. ]]\n",
      "layer derivative/slope: \n",
      " [[ 0.447  0.     0.     0.     0.     0.   ]\n",
      " [ 0.    -1.064  0.     0.     0.     0.   ]\n",
      " [ 0.     0.     1.319  0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.288  0.     0.   ]\n",
      " [ 0.     0.     0.     0.    -1.026  0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.906]]\n"
     ]
    }
   ],
   "source": [
    "print(\"prediction: \\n\", LRL.output)\n",
    "print(\"expected output: \\n\", lrl_target)\n",
    "print(\"derivative matrix: \\n\", np.around(LRL.mx, 3))\n",
    "print(\"expected derivative matrix: \\n\", lrl_slope)\n",
    "print(\"layer derivative/slope: \\n\", np.around(LRL.delta, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congragulations you have implemented your first fully functional neural network with a single layer.\n",
    "\n",
    "Now you have the correct structure, but the results should be mostly gibbrish. \n",
    "That is absolutely normal, because we have not yet seen how to **train** the neural network.\n",
    "\n",
    "\n",
    "We will see that in another tutorial, because I am too sleepy now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
